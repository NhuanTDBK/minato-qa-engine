{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from typing import List, Optional, Any\n",
    "import pickle\n",
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import jinja2\n",
    "import random\n",
    "import re\n",
    "from tqdm import trange, tqdm\n",
    "from pydantic import BaseModel, Field, TypeAdapter, RootModel, ValidationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_extract = pickle.load(open(\"wikimedia_extracted_data.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_guideline = \"\"\"\n",
    "- Both contents are at most 50 words\n",
    "- Do not use this or that in the response, instead use the name of the entity\n",
    "- Be creative, factfulness \\n\n",
    "\"\"\"\n",
    "default_prompt_template = \"\"\"\n",
    "### Instruction\n",
    "{instruction}\n",
    "### Guideline\n",
    "{guideline}\n",
    "### Input\n",
    "{input}\n",
    "### Response\n",
    "{response}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class InstructionOutput(BaseModel):\n",
    "    instruction: str = Field(\n",
    "        ..., title=\"Instruction\"\n",
    "    )\n",
    "    output: str = Field(..., title=\"Output\", description=\"Output of the instruction\")\n",
    "\n",
    "\n",
    "class InstructionInputOuput(BaseModel):\n",
    "    instruction: str = Field(\n",
    "        ..., title=\"Instruction\"\n",
    "    )\n",
    "    input: str = Field(..., title=\"Input\", description=\"Input to the instruction\")\n",
    "    output: str = Field(..., title=\"Output\", description=\"Output of the instruction\")\n",
    "\n",
    "\n",
    "class QuestionAnswering(BaseModel):\n",
    "    question: str = Field()\n",
    "    answer: str = Field()\n",
    "\n",
    "\n",
    "class MultipleChoiceQuestionAnswering(BaseModel):\n",
    "    question: str = Field()\n",
    "    choices: str = Field(\n",
    "        description=\"Multiple choices for the question with structure: A. Choice 1\\n B. Choice 2\\n C. Choice 3\\n D. Choice 4, etc.\"\n",
    "    )\n",
    "    # choices: List[Choices] = Field()\n",
    "    answer: str = Field(\n",
    "        description=\"Answer to the question, value should be single letter such as A, B, C, D, etc.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class EntityKeyValue(BaseModel):\n",
    "    entity_name: str = Field(..., title=\"Entity Name\", description=\"Name of the entity\")\n",
    "    entity_value: str = Field(\n",
    "        ..., title=\"Entity Value\", description=\"Value of the entity\"\n",
    "    )\n",
    "\n",
    "\n",
    "class EntityExtractionOutput(BaseModel):\n",
    "    instruction: str = Field(\n",
    "        ...,\n",
    "        title=\"Instruction\",\n",
    "    )\n",
    "    output: List[EntityKeyValue] = Field(\n",
    "        ..., title=\"Output\", description=\"Output of the instruction\"\n",
    "    )\n",
    "\n",
    "\n",
    "class PromptMeta(BaseModel):\n",
    "    name: Optional[str] = Field(\n",
    "        default=None, title=\"Name\", description=\"Name of the prompt\"\n",
    "    )\n",
    "    instruction: str = Field(\n",
    "        title=\"Instruction\"\n",
    "    )\n",
    "    guideline: Optional[str] = Field(\n",
    "        default=default_guideline,\n",
    "        title=\"Guideline\",\n",
    "        description=\"Guideline to be followed\",\n",
    "    )\n",
    "    input: Optional[str] = Field(\n",
    "        default=None, title=\"Input\", description=\"Input to the instruction\"\n",
    "    )\n",
    "    response: Optional[str] = Field(\n",
    "        default=None, title=\"Response\", description=\"Response to the instruction\"\n",
    "    )\n",
    "    expected_output: Optional[Any] = Field(\n",
    "        default=None,\n",
    "        title=\"Expected Output\",\n",
    "        description=\"Expected output of the instruction\",\n",
    "    )\n",
    "\n",
    "    def to_prompt(self) -> str:\n",
    "        return default_prompt_template.format(\n",
    "            instruction=self.instruction,\n",
    "            guideline=self.guideline,\n",
    "            input=self.input,\n",
    "            response=self.response,\n",
    "        )\n",
    "\n",
    "\n",
    "QuestionAnsweringOutput = RootModel[List[QuestionAnswering]]\n",
    "MultipleChoiceQuestionAnsweringOutput = RootModel[List[MultipleChoiceQuestionAnswering]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "YAML_FORMAT_INSTRUCTIONS = jinja2.Template(\"\"\"The output should be formatted as a YAML instance that conforms to the given JSON schema below.\n",
    "\n",
    "# Examples\n",
    "## Schema\n",
    "```\n",
    "{\"properties\": {\"habit\": { \"description\": \"A common daily habit\", \"type\": \"string\" }, \"sustainable_alternative\": { \"description\": \"An environmentally friendly alternative to the habit\", \"type\": \"string\"}}}, \"required\": [\"habit\", \"sustainable_alternative\"]}}\n",
    "```\n",
    "## Well formatted instance\n",
    "```\n",
    "habit: Using disposable water bottles for daily hydration\n",
    "sustainable_alternative: |\n",
    "    Fire Style: Majestic Flame Destroyer\n",
    "``` \n",
    "Please follow the standard YAML formatting conventions with correct indentations, and make sure that the data types adhere strictly to the following JSON schema: \n",
    "```\n",
    "{{schema}}\n",
    "```\n",
    "Always use block scalar literal style '|' in YAML if answers include special characters such as colons, dashs\n",
    "Make sure to always enclose the YAML output in triple backticks (```). Please do not add anything other than valid YAML output!\"\"\")\n",
    "\n",
    "JSON_FORMAT_INSTRUCTIONS = jinja2.Template(\"\"\"The output should be formatted as a JSON instance that conforms to the given JSON schema below.\n",
    "                                           \n",
    "# Examples\n",
    "## Schema\n",
    "```\n",
    "{\"properties\": {\"habit\": { \"description\": \"A common daily habit\", \"type\": \"string\" }, \"sustainable_alternative\": { \"description\": \"An environmentally friendly alternative to the habit\", \"type\": \"string\"}}}, \"required\": [\"habit\", \"sustainable_alternative\"]}}\n",
    "```\n",
    "## Well formatted instance\n",
    "```\n",
    "{\n",
    "    \"habit\": \"Using disposable water bottles for daily hydration\",\n",
    "    \"sustainable_alternative\": \"Fire Style: Majestic Flame Destroyer\"\n",
    "}\n",
    "```\n",
    "Please follow the standard JSON formatting conventions with correct indentations, and make sure that the data types adhere strictly to the following JSON schema:\n",
    "```\n",
    "{{schema}}\n",
    "```\n",
    "Use escape double quotes in a string is by using backslashes (\\)\n",
    "Make sure to always enclose the JSON output in triple backticks (```). Please do not add anything other than valid JSON output!\"\"\")\n",
    "\n",
    "def get_format_instructions(cls: BaseModel) -> str:\n",
    "    schema = cls.model_json_schema()\n",
    "\n",
    "    # Remove extraneous fields.\n",
    "    reduced_schema = schema\n",
    "    if \"title\" in reduced_schema:\n",
    "        del reduced_schema[\"title\"]\n",
    "    if \"type\" in reduced_schema:\n",
    "        del reduced_schema[\"type\"]\n",
    "    # Ensure yaml in context is well-formed with double quotes.\n",
    "    schema_str = json.dumps(reduced_schema)\n",
    "    return JSON_FORMAT_INSTRUCTIONS.render(schema=schema_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "11 24\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.3-70B-Instruct-Turbo\"\n",
    "api_key = \"M8lfE1LIxpckoMXmMi2uW1tzPW2kZAwI\"\n",
    "base_url = \"https://api.deepinfra.com/v1/openai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for InstructionOutput\ninstruction\n  Field required [type=missing, input_value={'type': 'function', 'nam...g', 'value': 'Hello!'}}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\noutput\n  Field required [type=missing, input_value={'type': 'function', 'nam...g', 'value': 'Hello!'}}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create an OpenAI client with your deepinfra token and endpoint\u001b[39;00m\n\u001b[1;32m      5\u001b[0m chat_openai_client \u001b[38;5;241m=\u001b[39m OpenAI(\n\u001b[1;32m      6\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[1;32m      7\u001b[0m     base_url\u001b[38;5;241m=\u001b[39mbase_url,\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m chat_completion \u001b[38;5;241m=\u001b[39m \u001b[43mchat_openai_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mInstructionOutput\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(chat_completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(chat_completion\u001b[38;5;241m.\u001b[39musage\u001b[38;5;241m.\u001b[39mprompt_tokens, chat_completion\u001b[38;5;241m.\u001b[39musage\u001b[38;5;241m.\u001b[39mcompletion_tokens)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/naruto_qa/lib/python3.9/site-packages/openai/resources/beta/chat/completions.py:160\u001b[0m, in \u001b[0;36mCompletions.parse\u001b[0;34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparser\u001b[39m(raw_completion: ChatCompletion) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ParsedChatCompletion[ResponseFormatT]:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_chat_completion(\n\u001b[1;32m    155\u001b[0m         response_format\u001b[38;5;241m=\u001b[39mresponse_format,\n\u001b[1;32m    156\u001b[0m         chat_completion\u001b[38;5;241m=\u001b[39mraw_completion,\n\u001b[1;32m    157\u001b[0m         input_tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m    158\u001b[0m     )\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_type_to_response_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\u001b[39;49;00m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# in the `parser` function above\u001b[39;49;00m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mType\u001b[49m\u001b[43m[\u001b[49m\u001b[43mParsedChatCompletion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseFormatT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/naruto_qa/lib/python3.9/site-packages/openai/_base_client.py:1283\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1271\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1280\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1281\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1282\u001b[0m     )\n\u001b[0;32m-> 1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/naruto_qa/lib/python3.9/site-packages/openai/_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/naruto_qa/lib/python3.9/site-packages/openai/_base_client.py:1066\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1063\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1066\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/naruto_qa/lib/python3.9/site-packages/openai/_base_client.py:1165\u001b[0m, in \u001b[0;36mSyncAPIClient._process_response\u001b[0;34m(self, cast_to, options, response, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(response\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(RAW_RESPONSE_HEADER)):\n\u001b[1;32m   1163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, api_response)\n\u001b[0;32m-> 1165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapi_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/naruto_qa/lib/python3.9/site-packages/openai/_response.py:319\u001b[0m, in \u001b[0;36mAPIResponse.parse\u001b[0;34m(self, to)\u001b[0m\n\u001b[1;32m    317\u001b[0m parsed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse(to\u001b[38;5;241m=\u001b[39mto)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_given(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options\u001b[38;5;241m.\u001b[39mpost_parser):\n\u001b[0;32m--> 319\u001b[0m     parsed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparsed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(parsed, BaseModel):\n\u001b[1;32m    322\u001b[0m     add_request_id(parsed, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_id)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/naruto_qa/lib/python3.9/site-packages/openai/resources/beta/chat/completions.py:154\u001b[0m, in \u001b[0;36mCompletions.parse.<locals>.parser\u001b[0;34m(raw_completion)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparser\u001b[39m(raw_completion: ChatCompletion) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ParsedChatCompletion[ResponseFormatT]:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse_chat_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchat_completion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_completion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_tools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/naruto_qa/lib/python3.9/site-packages/openai/lib/_parsing/_completions.py:110\u001b[0m, in \u001b[0;36mparse_chat_completion\u001b[0;34m(response_format, input_tools, chat_completion)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m                 tool_calls\u001b[38;5;241m.\u001b[39mappend(tool_call)\n\u001b[1;32m    103\u001b[0m     choices\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    104\u001b[0m         construct_type_unchecked(\n\u001b[1;32m    105\u001b[0m             type_\u001b[38;5;241m=\u001b[39mcast(Any, ParsedChoice)[solve_response_format_t(response_format)],\n\u001b[1;32m    106\u001b[0m             value\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    107\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mchoice\u001b[38;5;241m.\u001b[39mto_dict(),\n\u001b[1;32m    108\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    109\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmessage\u001b[38;5;241m.\u001b[39mto_dict(),\n\u001b[0;32m--> 110\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparsed\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mmaybe_parse_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    114\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_calls,\n\u001b[1;32m    115\u001b[0m                 },\n\u001b[1;32m    116\u001b[0m             },\n\u001b[1;32m    117\u001b[0m         )\n\u001b[1;32m    118\u001b[0m     )\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    121\u001b[0m     ParsedChatCompletion[ResponseFormatT],\n\u001b[1;32m    122\u001b[0m     construct_type_unchecked(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m     ),\n\u001b[1;32m    129\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/naruto_qa/lib/python3.9/site-packages/openai/lib/_parsing/_completions.py:161\u001b[0m, in \u001b[0;36mmaybe_parse_content\u001b[0;34m(response_format, message)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmaybe_parse_content\u001b[39m(\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    157\u001b[0m     response_format: \u001b[38;5;28mtype\u001b[39m[ResponseFormatT] \u001b[38;5;241m|\u001b[39m ResponseFormatParam \u001b[38;5;241m|\u001b[39m NotGiven,\n\u001b[1;32m    158\u001b[0m     message: ChatCompletionMessage \u001b[38;5;241m|\u001b[39m ParsedChatCompletionMessage[\u001b[38;5;28mobject\u001b[39m],\n\u001b[1;32m    159\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseFormatT \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_rich_response_format(response_format) \u001b[38;5;129;01mand\u001b[39;00m message\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m message\u001b[38;5;241m.\u001b[39mrefusal:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/naruto_qa/lib/python3.9/site-packages/openai/lib/_parsing/_completions.py:221\u001b[0m, in \u001b[0;36m_parse_content\u001b[0;34m(response_format, content)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_parse_content\u001b[39m(response_format: \u001b[38;5;28mtype\u001b[39m[ResponseFormatT], content: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseFormatT:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_basemodel_type(response_format):\n\u001b[0;32m--> 221\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseFormatT, \u001b[43mmodel_parse_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_dataclass_like_type(response_format):\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m PYDANTIC_V2:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/naruto_qa/lib/python3.9/site-packages/openai/_compat.py:169\u001b[0m, in \u001b[0;36mmodel_parse_json\u001b[0;34m(model, data)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmodel_parse_json\u001b[39m(model: \u001b[38;5;28mtype\u001b[39m[_ModelT], data: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _ModelT:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m PYDANTIC_V2:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_validate_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparse_raw(data)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/naruto_qa/lib/python3.9/site-packages/pydantic/main.py:656\u001b[0m, in \u001b[0;36mBaseModel.model_validate_json\u001b[0;34m(cls, json_data, strict, context)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    655\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for InstructionOutput\ninstruction\n  Field required [type=missing, input_value={'type': 'function', 'nam...g', 'value': 'Hello!'}}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\noutput\n  Field required [type=missing, input_value={'type': 'function', 'nam...g', 'value': 'Hello!'}}}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing"
     ]
    }
   ],
   "source": [
    "# Assume openai>=1.0.0\n",
    "from openai import OpenAI\n",
    "\n",
    "# Create an OpenAI client with your deepinfra token and endpoint\n",
    "chat_openai_client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url,\n",
    ")\n",
    "\n",
    "chat_completion = chat_openai_client.beta.chat.completions.parse(\n",
    "    model=model_name,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "    response_format=InstructionOutput\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)\n",
    "print(chat_completion.usage.prompt_tokens, chat_completion.usage.completion_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_instructions = [\n",
    "    {\n",
    "        \"name\": \"Information Extraction : Extracting people, location, events from text\",\n",
    "        \"instruction\": \"Generate an instruction and a output related to Information Extraction : Extracting people, location, events from text, in the context of Naruto manga, return in YAML format\",\n",
    "        \"level\": 1,\n",
    "        \"expected_output\": EntityExtractionOutput,\n",
    "        \"guideline\": \"\\nIgnore entity value empty\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Relation Extraction : Discovering and categorizing relationships between entities within a text\",\n",
    "        \"instruction\": \"Generate an instruction and a output related to Relation Extraction : Discovering and categorizing relationships between entities within a text, in the context of Naruto manga return in YAML format\",\n",
    "        \"level\": 1,\n",
    "        \"expected_output\": InstructionOutput,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Give me the definition of the words in this context about Naruto manga. Focus on noun term. Naruto manga has some influences from Buddhist and Japan culture of ninja, samurai\",\n",
    "        \"instruction\": \"Generate an instruction and a output related to Give me the definition of the words in this context about Naruto manga. Focus on noun term. Naruto manga has some influences from Buddhist and Japan culture of ninja, samurai, return in YAML format\",\n",
    "        \"level\": 1,\n",
    "        \"expected_output\": InstructionOutput,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Replace the <mask> token in the text with proper words that are consistent with the context. You can use multiple words for each <mask> token\",\n",
    "        \"instruction\": \"Generate an instruction, a input and a output related to Replace the MASK token in the text with proper words that are consistent with the context, return in YAML format\",\n",
    "        \"level\": 1,\n",
    "        \"expected_output\": InstructionInputOuput,\n",
    "        \"guideline\": \"Both contents contain at most 3 sentences, tokens are at most 100 words\\nBe creative, factfulness\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Verify if the claim is true or false based on provided context. It is false, explain why\",\n",
    "        \"instruction\": \"Generate an instruction and a output related to Verify if the claim is true or false based on provided context. It is false, explain why, return in YAML format\",\n",
    "        \"expected_output\": InstructionInputOuput,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Intent Recognition : Determining the author's intention or goal behind a given text\",\n",
    "        \"instruction\": \"Generate an instruction and a output related to Intent Recognition : Determining the author's intention or goal behind a given text based on a given text. Background: Naruto is fictional manga written by Japanese artist, which has some influences from Buddhist and Japan culture of ninja, samurai, return in YAML format\",\n",
    "        \"level\": 1,\n",
    "        \"expected_output\": InstructionOutput,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Translation : Converting text from English to Japanese\",\n",
    "        \"level\": 1,\n",
    "        \"instruction\": \"Translate input from English to Japanese\\n### Steps by steps \\n- Summarize the input into shorter paragraph.\\n- Translate summarized content to Japanese\\n- Check factualness, correct grammar\",\n",
    "        \"expected_output\": InstructionOutput,\n",
    "        \"guideline\": \"Output paragraph contains at most 3 sentences, tokens are at most 100 words\\nBe creative, factfulness\\nReturn in string, no further explained\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Translation : Converting text from English to Vietnamese\",\n",
    "        \"level\": 1,\n",
    "        \"instruction\": \"Translate input from English to Vietnamese\\n### Steps by steps \\n- Summarize the input into shorter paragraph.\\n- Translate summarized content to Vietnamese\\n- Check factualness, correct grammar\",\n",
    "        \"expected_output\": InstructionOutput,\n",
    "        \"guideline\": \"Output paragraph contains at most 4 sentences, tokens are at most 100 words\\nBe creative, factfulness\\nReturn in string, no further explained\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Text Summarization\",\n",
    "        \"instruction\": \"Create separate summaries for each subsection or topic within the given text\",\n",
    "        \"expected_output\": InstructionOutput,\n",
    "        \"guideline\": \"Output paragraph contains at most 3 sentences, tokens are at most 50 words\\nBe creative, factfulness\\nReturn in string, no further explained\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Text Summarization\",\n",
    "        \"instruction\": \"Produce a bullet-point summary highlighting the key facts from the passage\",\n",
    "        \"expected_output\": InstructionOutput,\n",
    "        \"guideline\": \"Output paragraph contains at most 3 sentences, tokens are at most 30 words\\nBe creative, factfulness\\nReturn in string, no further explained\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Text Summarization\",\n",
    "        \"instruction\": \"Write a short abstract summarizing the purpose, methods and findings in the text\",\n",
    "        \"expected_output\": InstructionOutput,\n",
    "        \"guideline\": \"Output paragraph contains at most 3 sentences, tokens are at most 30 words\\nBe creative, factfulness\\nReturn in string, no further explained\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Text Summarization\",\n",
    "        \"instruction\": \"Condense the given text into a summary while preserving the original tone/style\",\n",
    "        \"expected_output\": InstructionOutput,\n",
    "        \"guideline\": \"Output paragraph contains at most 3 sentences, tokens are at most 30 words\\nBe creative, factfulness\\nReturn in string, no further explained\",\n",
    "    },\n",
    "]\n",
    "\n",
    "number_of_instances = 10\n",
    "qa_instructions = [\n",
    "    {\n",
    "        \"name\": \"Reading Comprehension : Understanding and answering questions based on a given text\",\n",
    "        \"level\": 1,\n",
    "        \"instruction\": f\"Generate {number_of_instances} question and answer pairs related to Reading Comprehension : Understanding and answering questions based on a given text\",\n",
    "        \"expected_output\": QuestionAnsweringOutput,\n",
    "        \"guideline\": \"\"\"\n",
    "- Question about who, what, when, where, why and how questions about the given text\n",
    "- Question and answer tokens are at most 50 words\n",
    "- Do not use this or that in the question, use the explicit name of the entity\n",
    "- Difficulty level: easy. Definition of easy: The answer is directly in the text\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Reading Comprehension : Understanding and answering questions based on a given text\",\n",
    "        \"level\": 1,\n",
    "        \"instruction\": f\"Generate {number_of_instances} question and answer pairs related to Reading Comprehension : Understanding and answering questions based on a given text\",\n",
    "        \"expected_output\": QuestionAnsweringOutput,\n",
    "        \"guideline\": \"\"\"\n",
    "- Questions cover cover a range of topics, including character motivations, plot twists, specific dialogues, who, what, when, where, why and how questions about the given text\n",
    "- Question and answer tokens are at most 100 words\n",
    "- Do not use this or that in the question, use the explicit name of the entity\n",
    "- Draft questions that require in-depth knowledge and critical thinking, Avoid straightforward questions. Instead, focus on nuanced and detailed aspects of the manga\n",
    "- Consider aspects like character backgrounds, hidden abilities, and lesser-known facts.\n",
    "- Difficulty level: medium. Definition of medium: The answer is not directly in the text, but can be inferred from the text. For example, relationship between two people, or the cause of an event\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Reading Comprehension : Understanding and answering questions based on a given text\",\n",
    "        \"level\": 1,\n",
    "        \"instruction\": f\"Using the following paragraph, write {number_of_instances} multiple-choices question-answer pairs\",\n",
    "        \"expected_output\": MultipleChoiceQuestionAnsweringOutput,\n",
    "        \"guideline\": \"\"\"\n",
    "- Questions cover cover a range of topics, including character motivations, plot twists, specific dialogues, who, what, when, where, why and how questions about the given text\n",
    "- Question and answer tokens are at most 100 words\n",
    "- Do not use this or that in the question, use the explicit name of the entity\n",
    "- Difficulty level: hard. Definition of hard: The answer is not directly in the text, but can be inferred from the text and reasoning is required. There are at most 6 choices, and only one is correct, include None of the above are correct\n",
    "- Incorporate specific details from the context that require careful reading and recall\n",
    "- Include questions about the cultural and mythological references used in the context if possible\n",
    "- Pose questions that require the reader to connect different parts\n",
    "        \"\"\",\n",
    "\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_format_yaml(yaml_string):\n",
    "    corrected_yaml_string = yaml_string.replace(\"```yaml\", \"\").replace(\"```\", \"\")\n",
    "    return corrected_yaml_string\n",
    "\n",
    "\n",
    "def auto_format_json(json_string):\n",
    "    corrected_json_string = json_string.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "    return corrected_json_string\n",
    "\n",
    "\n",
    "def truncate_content(content, max_length=4096):\n",
    "    tokens = content.split()\n",
    "    if len(tokens) * 1.2 > max_length:\n",
    "        tokens = tokens[:max_length]\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "def call_llm(\n",
    "    chat_client: OpenAI,\n",
    "    model_name: str,\n",
    "    prompt,\n",
    "    structure_output=None,\n",
    "    raise_exception: bool = False,\n",
    "    **kwargs,\n",
    "):\n",
    "    for retry in range(getattr(chat_client, \"max_retries\", 3)):\n",
    "        try:\n",
    "            if structure_output:\n",
    "                response = chat_client.beta.chat.completions.parse(\n",
    "                    model=model_name,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    response_format=structure_output,\n",
    "                    **kwargs,\n",
    "                )\n",
    "                return response.choices[0].message.parsed\n",
    "            else:\n",
    "                response = chat_client.chat.completions.create(\n",
    "                    model=model_name,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    **kwargs,\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Retry {retry}: {str(e)}\")\n",
    "            if raise_exception:\n",
    "                raise e\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(create_prompt_from_meta(\n",
    "#     meta_instruction=qa_instructions[1],\n",
    "#     input_content=sportseekda_prompts[5].input,\n",
    "# ).to_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_from_meta(meta_instruction, input_content: str):\n",
    "    return PromptMeta(\n",
    "        name=meta_instruction[\"name\"],\n",
    "        instruction=meta_instruction[\"instruction\"],\n",
    "        guideline=meta_instruction.get(\"guideline\", default_guideline)\n",
    "        + \"\\n\"\n",
    "        + get_format_instructions(meta_instruction[\"expected_output\"]),\n",
    "        input=input_content,\n",
    "        response=meta_instruction.get(\"response\", \"\"),\n",
    "        expected_output=meta_instruction[\"expected_output\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def create_synthetic_instructions_from_passage(\n",
    "    chat_client: OpenAI,\n",
    "    model_name: str,\n",
    "    content: str,\n",
    "    prompts: List[dict],\n",
    "    random_instructions=2,\n",
    "    multiple_samples=3,\n",
    "):\n",
    "    content = truncate_content(content)\n",
    "    meta_prompts: List[PromptMeta] = [\n",
    "        create_prompt_from_meta(meta_instruction, content)\n",
    "        for meta_instruction in prompts\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    random_instructions = min(random_instructions, len(meta_instructions))\n",
    "    prompts_to_use = random.sample(meta_prompts, k=random_instructions)\n",
    "    for prompt_base in prompts_to_use:\n",
    "        for _ in range(multiple_samples):\n",
    "            response = call_llm(\n",
    "                chat_client=chat_client,\n",
    "                model_name=model_name,\n",
    "                prompt=prompt_base.to_prompt(),\n",
    "                raise_exception=False,\n",
    "                structure_output=prompt_base.expected_output,\n",
    "            )\n",
    "            new_prompt_base = PromptMeta(**prompt_base.model_dump())\n",
    "            new_prompt_base.response = response\n",
    "            results.append(new_prompt_base)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def create_synthetic_instructions_from_wikipage(\n",
    "    chat_client: OpenAI,\n",
    "    model_name: str,\n",
    "    wiki_extract_page,\n",
    "    topic,\n",
    "    prompts: List[dict],\n",
    "    random_instructions=2,\n",
    "    multiple_samples=3,\n",
    "):\n",
    "    content = truncate_content(wiki_extract_page[\"content_headings\"][topic].strip())\n",
    "    return create_synthetic_instructions_from_passage(\n",
    "        chat_client=chat_client,\n",
    "        model_name=model_name,\n",
    "        content=content,\n",
    "        prompts=prompts,\n",
    "        random_instructions=random_instructions,\n",
    "        multiple_samples=multiple_samples,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_synthetic_prompts(synthetic_instructions: List[PromptMeta]):\n",
    "    res = []\n",
    "    return_format = [\n",
    "        {\n",
    "            \"guideline\": \"Return in YAML format\",\n",
    "            \"serializer\": yaml.dump,\n",
    "        },\n",
    "        {\n",
    "            \"guideline\": \"Return in JSON format\",\n",
    "            \"serializer\": json.dumps,\n",
    "        },\n",
    "    ]\n",
    "    for synthetic_instruction in synthetic_instructions:\n",
    "        try:\n",
    "            return_format_option = random.choice(return_format)\n",
    "            cls_output: BaseModel = synthetic_instruction.expected_output\n",
    "            cls_obj = cls_output.model_validate(synthetic_instruction.response)\n",
    "            cls_dict = cls_obj.model_dump()\n",
    "\n",
    "            response = (\n",
    "                cls_dict[\"output\"]\n",
    "                if isinstance(cls_dict[\"output\"], str)\n",
    "                else return_format_option[\"serializer\"](cls_dict[\"output\"])\n",
    "            )\n",
    "\n",
    "            res.append(\n",
    "                PromptMeta(\n",
    "                    instruction=cls_dict[\"instruction\"],\n",
    "                    guideline=return_format_option[\"guideline\"],\n",
    "                    input=cls_dict.get(\"input\", synthetic_instruction.input),\n",
    "                    response=response,\n",
    "                )\n",
    "            )\n",
    "        except ValidationError as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    return res\n",
    "\n",
    "\n",
    "def create_synthetic_qa_prompts(\n",
    "    qa_response_prompts: List[PromptMeta], instruction: str = \"Answer the question\"\n",
    "):\n",
    "    res = []\n",
    "    for qa_response_prompt in qa_response_prompts:\n",
    "        try:\n",
    "            cls_output: BaseModel = qa_response_prompt.expected_output\n",
    "            list_of_qa_obj: list = cls_output.model_validate(\n",
    "                qa_response_prompt.response\n",
    "            )\n",
    "            list_of_qa = list_of_qa_obj.dict()\n",
    "\n",
    "            for qa_obj in list_of_qa:\n",
    "                response = qa_obj[\"answer\"].strip()\n",
    "                input_content = (\n",
    "                    qa_obj[\"question\"].strip() + \"\\n\" + qa_obj.get(\"choices\", \"\")\n",
    "                )\n",
    "                res.append(\n",
    "                    PromptMeta(\n",
    "                        instruction=instruction,\n",
    "                        guideline=\"\",\n",
    "                        input=input_content,\n",
    "                        response=response,\n",
    "                    )\n",
    "                )\n",
    "        except ValidationError as e:\n",
    "            print(e, qa_response_prompt.response)\n",
    "            continue\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = create_synthetic_instructions_from_wikipage(\n",
    "    chat_client=chat_openai_client,\n",
    "    model_name=model_name,\n",
    "    wiki_extract_page=wiki_extract[121],\n",
    "    topic=\"Appearance\",\n",
    "    prompts=meta_instructions,\n",
    "    random_instructions=1,\n",
    "    multiple_samples=2,\n",
    ")\n",
    "# t = create_synthetic_prompts(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InstructionOutput(instruction='Extract relationship between Son Gok and Sage of Six Paths', output='Son Gok was known to the Sage of Six Paths during its smaller and leaner days')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0].response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = create_synthetic_prompts(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_instructions = 2\n",
    "multiple_samples = 1\n",
    "\n",
    "\n",
    "def generate_and_write_synthetic_prompts(\n",
    "    client: OpenAI,\n",
    "    model_name: str,\n",
    "    data: List,\n",
    "    output_file_name: str,\n",
    "    multiple_samples: int,\n",
    "    random_instructions: int,\n",
    "):\n",
    "    count = 1\n",
    "    for idx in trange(len(data)):\n",
    "        for topic in tqdm(data[idx][\"content_headings\"].keys(), desc=\"Topic\"):\n",
    "            response = create_synthetic_instructions_from_wikipage(\n",
    "                chat_client=client,\n",
    "                wiki_extract_page=data[idx],\n",
    "                model_name=model_name,\n",
    "                topic=topic,\n",
    "                prompts=meta_instructions,\n",
    "                random_instructions=random_instructions,\n",
    "                multiple_samples=multiple_samples,\n",
    "            )\n",
    "            synthetic_prompts = create_synthetic_prompts(response)\n",
    "            with open(output_file_name, \"a+\") as f:\n",
    "                for prompt in synthetic_prompts:\n",
    "                    f.write(prompt.json() + \"\\n\")\n",
    "                    count += 1\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_extract_sample = random.sample(wiki_extract, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Topic:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "create_synthetic_instructions_from_wikipage() got an unexpected keyword argument 'model_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m random_instructions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      2\u001b[0m multiple_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mgenerate_and_write_synthetic_prompts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat_openai_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwiki_extract_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msynthetic_prompts.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultiple_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiple_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_instructions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_instructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[46], line 15\u001b[0m, in \u001b[0;36mgenerate_and_write_synthetic_prompts\u001b[0;34m(client, data, output_file_name, multiple_samples, random_instructions)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;28mlen\u001b[39m(data)):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m tqdm(data[idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_headings\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys(), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopic\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 15\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_synthetic_instructions_from_wikipage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchat_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwiki_extract_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtopic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtopic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta_instructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrandom_instructions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_instructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmultiple_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiple_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m         synthetic_prompts \u001b[38;5;241m=\u001b[39m create_synthetic_prompts(response)\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma+\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mTypeError\u001b[0m: create_synthetic_instructions_from_wikipage() got an unexpected keyword argument 'model_name'"
     ]
    }
   ],
   "source": [
    "random_instructions = 3\n",
    "multiple_samples = 1\n",
    "\n",
    "\n",
    "generate_and_write_synthetic_prompts(\n",
    "    client=chat_openai_client,\n",
    "    model_name=model_name,\n",
    "    data=wiki_extract_sample,\n",
    "    output_file_name=\"synthetic_prompts.jsonl\",\n",
    "    multiple_samples=multiple_samples,\n",
    "    random_instructions=random_instructions,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate QA reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wiki_extract' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[253], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m                     f\u001b[38;5;241m.\u001b[39mwrite(prompt\u001b[38;5;241m.\u001b[39mjson() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m                     count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 28\u001b[0m wiki_extract_sample \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[43mwiki_extract\u001b[49m, \u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m     29\u001b[0m random_instructions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     30\u001b[0m multiple_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wiki_extract' is not defined"
     ]
    }
   ],
   "source": [
    "random_instructions = 2\n",
    "multiple_samples = 1\n",
    "\n",
    "\n",
    "def generate_and_write_question_answer_prompts(\n",
    "    client: ChatOpenAI,\n",
    "    data: List[PromptMeta],\n",
    "    output_file_name: str,\n",
    "    multiple_samples: int,\n",
    "    random_instructions: int,\n",
    "):\n",
    "    count = 1\n",
    "    for idx in trange(len(data)):\n",
    "        for topic in tqdm(data[idx][\"content_headings\"].keys(), desc=\"Topic\"):\n",
    "            response = create_synthetic_instructions_from_wikipage(\n",
    "                wiki_extract_page=data[idx],\n",
    "                chat_client=client,\n",
    "                topic=topic,\n",
    "                prompts=qa_instructions,\n",
    "                random_instructions=random_instructions,\n",
    "                multiple_samples=multiple_samples,\n",
    "            )\n",
    "            synthetic_prompts = create_synthetic_qa_prompts(response)\n",
    "            with open(output_file_name, \"a+\") as f:\n",
    "                for prompt in synthetic_prompts:\n",
    "                    f.write(prompt.json() + \"\\n\")\n",
    "                    count += 1\n",
    "wiki_extract_sample = random.sample(wiki_extract, 50)\n",
    "random_instructions = 2\n",
    "multiple_samples = 1\n",
    "\n",
    "generate_and_write_question_answer_prompts(\n",
    "    data=wiki_extract_sample,\n",
    "    fname=\"synthetic_qa_prompts.jsonl\",\n",
    "    multiple_samples=multiple_samples,\n",
    "    random_instructions=random_instructions,\n",
    ")                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load from sportseekeda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./prompt_sportseeker_v4.jsonl\") as f:\n",
    "    sportseekda_prompts = [PromptMeta.model_validate_json(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"bartowski/gemma-2-27b-it-GGUF/gemma-2-27b-it-Q4_K_S.gguf\"\n",
    "# client = ChatOpenAI(\n",
    "#     base_url=\"http://localhost:1234/v1\",\n",
    "#     api_key=\"lm_studio\",\n",
    "#     temperature=0.8,\n",
    "#     model=model_name,\n",
    "#     max_tokens=2048,\n",
    "#     n=2,\n",
    "#     max_retries=1,\n",
    "# )\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "client = ChatOpenAI(\n",
    "    base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    "    api_key=\"M8lfE1LIxpckoMXmMi2uW1tzPW2kZAwI\",\n",
    "    temperature=0.5,\n",
    "    model=model_name,\n",
    "    max_tokens=2048,\n",
    "    n=1,\n",
    "    max_retries=2,\n",
    "    model_kwargs={\n",
    "        \"top_p\": 0.95\n",
    "    },\n",
    ")\n",
    "# response = create_synthetic_instructions_from_passage(\n",
    "#     chat_client=client,\n",
    "#     content=sportseekda_prompts[7].input,\n",
    "#     prompts=[qa_instructions[1]],\n",
    "#     random_instructions=1,\n",
    "#     multiple_samples=1,\n",
    "# )\n",
    "# t = create_synthetic_qa_prompts(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total input tokens from qa_instruction and meta_instruction on all sportseekda_prompts\n",
    "total_prompts = []\n",
    "for i in range(len(sportseekda_prompts)):\n",
    "    for meta_instruction in qa_instructions:\n",
    "        p = create_prompt_from_meta(\n",
    "                meta_instruction=meta_instruction,\n",
    "                input_content=sportseekda_prompts[i].input,\n",
    "            ).to_prompt()\n",
    "        total_prompts.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4-turbo\")\n",
    "pricing = {\n",
    "    \"gpt-turbo\": {\n",
    "        \"input_price_per_million\": 0.75,\n",
    "        \"output_price_per_million\": 1\n",
    "    },\n",
    "    \"deepinfra_llama3-70b\": {\n",
    "        \"input_price_per_million\": 0.52,\n",
    "        \"output_price_per_million\": 0.75\n",
    "    }\n",
    "}\n",
    "\n",
    "total_input_tokens = sum([len(encoding.encode(p)) for p in total_prompts])\n",
    "average_output_tokens = 2048\n",
    "total_output_tokens = average_output_tokens * len(total_prompts)\n",
    "pricing[\"deepinfra_llama3-70b\"][\"input_price_per_million\"] * total_input_tokens / 1e6 + pricing[\"deepinfra_llama3-70b\"][\"output_price_per_million\"] * total_output_tokens / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.293706"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_instructions = 2\n",
    "multiple_samples = 1\n",
    "\n",
    "\n",
    "def generate_and_write_synthetic_prompts(\n",
    "    client: ChatOpenAI,\n",
    "    data: List[PromptMeta],\n",
    "    output_file_name: str,\n",
    "    multiple_samples: int,\n",
    "    random_instructions: int,\n",
    "):\n",
    "    count = 1\n",
    "    for idx in trange(len(data)):\n",
    "        response = create_synthetic_instructions_from_passage(\n",
    "            chat_client=client,\n",
    "            content=data[idx].input,\n",
    "            prompts=meta_instructions,\n",
    "            random_instructions=random_instructions,\n",
    "            multiple_samples=multiple_samples,\n",
    "        )\n",
    "        synthetic_prompts = create_synthetic_prompts(response)\n",
    "        with open(output_file_name, \"a+\") as f:\n",
    "            for prompt in synthetic_prompts:\n",
    "                f.write(prompt.json() + \"\\n\")\n",
    "                count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "def generate_and_write_qa_synthetic_prompts(\n",
    "    client: ChatOpenAI,\n",
    "    data: List[PromptMeta],\n",
    "    output_file_name: str,\n",
    "    multiple_samples: int,\n",
    "    random_instructions: int,\n",
    "):\n",
    "    count = 1\n",
    "    for idx in trange(len(data)):\n",
    "        response = create_synthetic_instructions_from_passage(\n",
    "            chat_client=client,\n",
    "            content=data[idx].input,\n",
    "            prompts=qa_instructions,\n",
    "            random_instructions=random_instructions,\n",
    "            multiple_samples=multiple_samples,\n",
    "        )\n",
    "        synthetic_prompts = create_synthetic_qa_prompts(response)\n",
    "        with open(output_file_name, \"a+\") as f:\n",
    "            for prompt in synthetic_prompts:\n",
    "                f.write(prompt.json() + \"\\n\")\n",
    "                count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_prompts = random.choices(sportseekda_prompts, k=20)\n",
    "# subset = set([sub_prompts[i].input for i in range(20)])\n",
    "# sub_prompts = [p for p in sportseekda_prompts if p.input not in subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|         | 9/131 [02:45<33:41, 16.57s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 validation error for InstructionOutput\n",
      "output\n",
      "  Input should be a valid string [type=string_type, input_value={'Yamanaka Clan': 'A nobl... to the Yamanaka Clan.'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.7/v/string_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 14/131 [03:59<30:32, 15.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Extra data: line 8 column 1 (char 227)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 16/131 [04:53<38:33, 20.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Extra data: line 9 column 1 (char 436)\n",
      "1 Extra data: line 8 column 1 (char 435)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|        | 17/131 [05:39<52:52, 27.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 validation error for InstructionOutput\n",
      "  Input should be a valid dictionary or instance of InstructionOutput [type=model_type, input_value=AIMessage(content='```\\n{...de-a589-df89e781f7b2-0'), input_type=AIMessage]\n",
      "    For further information visit https://errors.pydantic.dev/2.7/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|        | 18/131 [05:53<45:01, 23.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Extra data: line 9 column 1 (char 249)\n",
      "1 Extra data: line 8 column 1 (char 252)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|        | 19/131 [06:46<1:00:52, 32.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 validation error for InstructionOutput\n",
      "  Input should be a valid dictionary or instance of InstructionOutput [type=model_type, input_value=AIMessage(content='```\\n{...fb-afd5-e034b84cc79e-0'), input_type=AIMessage]\n",
      "    For further information visit https://errors.pydantic.dev/2.7/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|        | 20/131 [06:58<48:35, 26.27s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Extra data: line 8 column 1 (char 263)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|        | 26/131 [08:52<30:15, 17.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Invalid control character at: line 4 column 15 (char 177)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 33/131 [10:55<25:27, 15.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 validation error for InstructionOutput\n",
      "output\n",
      "  Input should be a valid string [type=string_type, input_value={'Uzumaki Clan Overview':...life force and Chakra.'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.7/v/string_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|       | 35/131 [11:16<20:53, 13.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Extra data: line 7 column 1 (char 374)\n",
      "1 Extra data: line 7 column 1 (char 482)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|       | 36/131 [11:57<33:59, 21.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 validation error for InstructionOutput\n",
      "  Input should be a valid dictionary or instance of InstructionOutput [type=model_type, input_value=AIMessage(content='```\\n{...51-8cad-18e866028449-0'), input_type=AIMessage]\n",
      "    For further information visit https://errors.pydantic.dev/2.7/v/model_type\n",
      "0 Extra data: line 9 column 1 (char 555)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 37/131 [13:34<1:08:50, 43.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Extra data: line 9 column 1 (char 487)\n",
      "1 validation error for InstructionOutput\n",
      "output\n",
      "  Input should be a valid string [type=string_type, input_value={'Ninja': 'A covert agent...ield the Wood Release.'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.7/v/string_type\n",
      "1 validation error for InstructionOutput\n",
      "  Input should be a valid dictionary or instance of InstructionOutput [type=model_type, input_value=AIMessage(content='```\\n{...c5-ba98-a5c78c31815d-0'), input_type=AIMessage]\n",
      "    For further information visit https://errors.pydantic.dev/2.7/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|      | 42/131 [14:48<27:08, 18.30s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Invalid control character at: line 4 column 17 (char 181)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 43/131 [15:10<28:28, 19.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 validation error for InstructionOutput\n",
      "output\n",
      "  Input should be a valid string [type=string_type, input_value={'Ninja': 'A covert agent... in the Naruto series.'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.7/v/string_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|      | 44/131 [15:28<27:20, 18.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Invalid control character at: line 4 column 79 (char 263)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|      | 45/131 [16:21<41:45, 29.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Invalid control character at: line 4 column 79 (char 263)\n",
      "1 validation error for InstructionOutput\n",
      "  Input should be a valid dictionary or instance of InstructionOutput [type=model_type, input_value=AIMessage(content='```\\n{...db-9f06-4e9d99cb7510-0'), input_type=AIMessage]\n",
      "    For further information visit https://errors.pydantic.dev/2.7/v/model_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|      | 46/131 [16:31<33:07, 23.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Extra data: line 7 column 1 (char 201)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|      | 47/131 [17:37<50:59, 36.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 validation error for InstructionOutput\n",
      "output\n",
      "  Input should be a valid string [type=string_type, input_value={'background': 'Yurui is ...e in the second stage.\"}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.7/v/string_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|      | 47/131 [2:22:45<4:15:07, 182.24s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[358], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sub_prompts \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoices(sportseekda_prompts, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mgenerate_and_write_synthetic_prompts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msub_prompts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m69\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msportseeka_synthetic_prompts.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultiple_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_instructions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# generate_and_write_qa_synthetic_prompts(\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#     client=client,\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#     data=sportseekda_prompts[270+20:],\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#     random_instructions=3,\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[240], line 14\u001b[0m, in \u001b[0;36mgenerate_and_write_synthetic_prompts\u001b[0;34m(client, data, output_file_name, multiple_samples, random_instructions)\u001b[0m\n\u001b[1;32m     12\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;28mlen\u001b[39m(data)):\n\u001b[0;32m---> 14\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_synthetic_instructions_from_passage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchat_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta_instructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_instructions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_instructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmultiple_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiple_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     synthetic_prompts \u001b[38;5;241m=\u001b[39m create_synthetic_prompts(response)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma+\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[283], line 32\u001b[0m, in \u001b[0;36mcreate_synthetic_instructions_from_passage\u001b[0;34m(chat_client, content, prompts, random_instructions, multiple_samples)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt_base \u001b[38;5;129;01min\u001b[39;00m prompts_to_use:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(multiple_samples):\n\u001b[0;32m---> 32\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mcall_llm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchat_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_base\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m            \u001b[49m\u001b[43mraise_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m         new_prompt_base \u001b[38;5;241m=\u001b[39m PromptMeta(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprompt_base\u001b[38;5;241m.\u001b[39mmodel_dump())\n\u001b[1;32m     38\u001b[0m         new_prompt_base\u001b[38;5;241m.\u001b[39mresponse \u001b[38;5;241m=\u001b[39m response\n",
      "Cell \u001b[0;32mIn[282], line 23\u001b[0m, in \u001b[0;36mcall_llm\u001b[0;34m(chat_client, prompt, raise_exception)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m retry \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(chat_client\u001b[38;5;241m.\u001b[39mmax_retries):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m         raw_text \u001b[38;5;241m=\u001b[39m \u001b[43mchat_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;66;03m# content = auto_format_yaml(raw_text.content)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;66;03m# content = yaml.safe_load(content)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m         content \u001b[38;5;241m=\u001b[39m auto_format_json(raw_text\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/semantic_search/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:170\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    166\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    167\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    169\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 170\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    180\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/semantic_search/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:599\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    593\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    598\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/semantic_search/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:456\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    455\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    457\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    458\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    460\u001b[0m ]\n\u001b[1;32m    461\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/semantic_search/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:446\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 446\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m         )\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/semantic_search/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:671\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 671\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    675\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/semantic_search/lib/python3.10/site-packages/langchain_community/chat_models/openai.py:442\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    437\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream} \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    441\u001b[0m }\n\u001b[0;32m--> 442\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/semantic_search/lib/python3.10/site-packages/langchain_community/chat_models/openai.py:357\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(\u001b[38;5;28mself\u001b[39m, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m    361\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/semantic_search/lib/python3.10/site-packages/openai/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/semantic_search/lib/python3.10/site-packages/openai/resources/chat/completions.py:590\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/semantic_search/lib/python3.10/site-packages/openai/_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1228\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1235\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1237\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1238\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1239\u001b[0m     )\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/semantic_search/lib/python3.10/site-packages/openai/_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/semantic_search/lib/python3.10/site-packages/openai/_base_client.py:952\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    949\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 952\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    958\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/semantic_search/lib/python3.10/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    906\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    910\u001b[0m )\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/semantic_search/lib/python3.10/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/semantic_search/lib/python3.10/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/semantic_search/lib/python3.10/site-packages/httpx/_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1019\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/semantic_search/lib/python3.10/site-packages/httpx/_transports/default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    231\u001b[0m )\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    238\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    239\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    240\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    241\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    242\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/semantic_search/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/semantic_search/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/semantic_search/lib/python3.10/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/semantic_search/lib/python3.10/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/semantic_search/lib/python3.10/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/semantic_search/lib/python3.10/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/semantic_search/lib/python3.10/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/semantic_search/lib/python3.10/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/semantic_search/lib/python3.10/ssl.py:1258\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1255\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1256\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1257\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/semantic_search/lib/python3.10/ssl.py:1131\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sub_prompts = random.choices(sportseekda_prompts, k=200)\n",
    "generate_and_write_synthetic_prompts(\n",
    "    client=client,\n",
    "    data=sub_prompts[69:],\n",
    "    output_file_name=\"sportseeka_synthetic_prompts.jsonl\",\n",
    "    multiple_samples=1,\n",
    "    random_instructions=2,\n",
    ")\n",
    "# generate_and_write_qa_synthetic_prompts(\n",
    "#     client=client,\n",
    "#     data=sportseekda_prompts[270+20:],\n",
    "#     output_file_name=\"sportseeka_synthetic_qa_prompts.jsonl\",\n",
    "#     multiple_samples=1,\n",
    "#     random_instructions=3,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|    | 11/20 [23:54<19:57, 133.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Expecting ',' delimiter: line 41 column 1 (char 1592)\n",
      "0 Expecting ',' delimiter: line 41 column 1 (char 1951)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 12/20 [27:50<21:54, 164.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Expecting ',' delimiter: line 42 column 1 (char 2880)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|   | 14/20 [32:28<14:57, 149.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Expecting ',' delimiter: line 41 column 1 (char 1383)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [47:11<00:00, 141.59s/it]\n"
     ]
    }
   ],
   "source": [
    "generate_and_write_qa_synthetic_prompts(\n",
    "    client=client,\n",
    "    data=sub_prompts,\n",
    "    output_file_name=\"sportseeka_qa_synthetic_prompts.jsonl\",\n",
    "    multiple_samples=1,\n",
    "    random_instructions=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_write_qa_synthetic_prompts(\n",
    "    client=client,\n",
    "    data=sub_prompts,\n",
    "    output_file_name=\"sportseeka_qa_synthetic_prompts.jsonl\",\n",
    "    multiple_samples=1,\n",
    "    random_instructions=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify by reward modelling and duplication detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.dedup import deduplicated_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9965\n"
     ]
    }
   ],
   "source": [
    "with open(\"sportseeka_synthetic_qa_prompts.jsonl\", \"r\") as f:\n",
    "    prompts = [json.loads(l) for l in f.readlines()]\n",
    "input_prompts = [prompt[\"input\"] for prompt in prompts]\n",
    "print(len(prompts))\n",
    "deduplicated_indices = deduplicated_contents(input_prompts, return_indices=True, ngrams=10, num_perm=64)\n",
    "deduplicated_prompts = [prompts[i] for i in deduplicated_indices]\n",
    "with open(\"deduplicated_synthetic_qa_prompts.jsonl\", \"w\") as f:\n",
    "    for prompt in deduplicated_prompts:\n",
    "        f.write(json.dumps(prompt) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpaths = [\n",
    "    # \"prompt_wiki_with_chunk_v5_512.jsonl\",\n",
    "    # \"prompt_wiki_with_chunk_v5_1024.jsonl\",\n",
    "    # \"prompt_wiki_with_chunk_v5_2048.jsonl\",\n",
    "]\n",
    "\n",
    "for fpath in fpaths:\n",
    "    with open(fpath) as f:\n",
    "        prompts = [json.loads(l) for l in f.readlines()]\n",
    "\n",
    "    input_prompts = [prompt[\"input\"] for prompt in prompts]\n",
    "    deduplicated_indices = deduplicated_contents(input_prompts, return_indices=True, ngrams=10, threshold=0.7, num_perm=256)\n",
    "    deduplicated_prompts = [prompts[i] for i in deduplicated_indices]\n",
    "    with open(f\"deduplicated_{fpath}\", \"w\") as f:\n",
    "        for prompt in deduplicated_prompts:\n",
    "            f.write(json.dumps(prompt) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prompt_sportseeker_v4.jsonl\") as f:\n",
    "    prompts = [json.loads(l) for l in f.readlines()]\n",
    "\n",
    "input_prompts = [prompt[\"input\"] for prompt in prompts]\n",
    "deduplicated_indices = deduplicated_contents(input_prompts, return_indices=True, ngrams=10, threshold=0.7, num_perm=256)\n",
    "deduplicated_prompts = [prompts[i] for i in deduplicated_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6521"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(deduplicated_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import datasets\n",
    "import json\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import trange\n",
    "model_path = \"OpenAssistant/reward-model-deberta-v3-large-v2\"\n",
    "model_path = \"sileod/deberta-v3-large-tasksource-rlhf-reward-model\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "pipe = pipeline(model=model, tokenizer=tokenizer, task=\"text-classification\", device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_template = \"\"\"\n",
    "### Instruction\n",
    "{instruction}\n",
    "### Input\n",
    "{input}\n",
    "### Response\n",
    "{response}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpaths = [\n",
    "    \"deduplicated_synthetic_qa_prompts.jsonl\",\n",
    "    # \"synthetic_prompts.jsonl\",\n",
    "]\n",
    "for fpath in fpaths:\n",
    "    with open(fpath) as f:\n",
    "        prompts = [json.loads(l) for l in f.readlines()]\n",
    "    # scores = pipe(\n",
    "    #     [\n",
    "    #         default_template.format(\n",
    "    #             instruction=prompts[i][\"instruction\"],\n",
    "    #             input=prompts[i][\"input\"],\n",
    "    #             response=prompts[i][\"response\"],\n",
    "    #         )\n",
    "    #         for i in range(len(prompts))\n",
    "    #     ],\n",
    "    #     max_length=512,\n",
    "    #     truncation=True,\n",
    "    #     batch_size=128,\n",
    "    # )\n",
    "    # # Get idx of low quality prompts\n",
    "    threshold = 0.1\n",
    "    with open(f\"high_quality_{fpath}\", \"w\") as f:\n",
    "        for prompt, score in zip(prompts, scores):\n",
    "            if score[\"score\"] > threshold:\n",
    "                f.write(json.dumps(prompt) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get idx of low quality prompts\n",
    "low_quality_prompts = [(prompts[i], s[\"score\"]) for i, s in enumerate(scores) if s[\"score\"] < 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'name': None,\n",
       "   'instruction': 'Answer the question',\n",
       "   'guideline': '',\n",
       "   'input': 'What can Black Zetsu do while controlling a target with Body Coating?\\n',\n",
       "   'response': \"While possessing a target's body, Black Zetsu can utilize their techniques, Kekkei Genkai, and any other abilities they might have. He can also control the victim and stabilize their life force.\",\n",
       "   'expected_output': None},\n",
       "  0.09773506224155426),\n",
       " ({'name': None,\n",
       "   'instruction': 'Answer the question',\n",
       "   'guideline': '',\n",
       "   'input': 'Can Earth Release be used to inflict harm on enemies?\\n',\n",
       "   'response': 'Yes, Earth Release can be broken up and turned into weapons to inflict harm on their enemies.',\n",
       "   'expected_output': None},\n",
       "  0.05670599266886711),\n",
       " ({'name': None,\n",
       "   'instruction': 'Answer the question',\n",
       "   'guideline': '',\n",
       "   'input': 'What is the difference between One-Finger Nukite and other variants of it?\\n',\n",
       "   'response': 'The potency and destructive power are increased as the number of fingers decreases. One-Finger Nukite, also known as the One Fingered Assault, is the deadliest of these variants.',\n",
       "   'expected_output': None},\n",
       "  0.06255616247653961),\n",
       " ({'name': None,\n",
       "   'instruction': 'Answer the question',\n",
       "   'guideline': '',\n",
       "   'input': 'What happens when Ino uses her Mind Transfer Jutsu on Sakura during the Chunin Exams?\\n',\n",
       "   'response': 'Ino switches bodies with Sakura and looks at answers she wrote, allowing her to cheat easily.',\n",
       "   'expected_output': None},\n",
       "  0.051265981048345566),\n",
       " ({'name': None,\n",
       "   'instruction': 'Answer the question',\n",
       "   'guideline': '',\n",
       "   'input': 'Can Substitution Jutsu be used as a weapon by attaching explosives to the substitute object?\\n',\n",
       "   'response': 'Yes, it is possible to use Substitution Jutsu as a weapon by attaching paper bombs or other explosives to the substitute object.',\n",
       "   'expected_output': None},\n",
       "  0.04203569144010544),\n",
       " ({'name': None,\n",
       "   'instruction': 'Answer the question',\n",
       "   'guideline': '',\n",
       "   'input': 'What does Aho mostly say when it flies around Naruto or other characters in the Naruto anime series?\\n',\n",
       "   'response': 'Aho mostly says Baka Baka (Idiot).',\n",
       "   'expected_output': None},\n",
       "  0.07113973051309586),\n",
       " ({'name': None,\n",
       "   'instruction': 'Answer the question',\n",
       "   'guideline': '',\n",
       "   'input': 'What is the name of the forbidden technique used by Tsuchigumo clan?\\n',\n",
       "   'response': 'The forbidden technique used by Tsuchigumo clan is called Fury, which can annihilate a whole village with an explosion.',\n",
       "   'expected_output': None},\n",
       "  0.06457583606243134),\n",
       " ({'name': None,\n",
       "   'instruction': 'Answer the question',\n",
       "   'guideline': '',\n",
       "   'input': 'What did Akane do to Inari when they were children?\\n',\n",
       "   'response': 'Akane stole Pochi from Inari, got bored of him, and threw him in the water.',\n",
       "   'expected_output': None},\n",
       "  0.049827639013528824),\n",
       " ({'name': None,\n",
       "   'instruction': 'Answer the question',\n",
       "   'guideline': '',\n",
       "   'input': 'Who helped Amachi steal a cargo ship that the Land of Water sent to the Land of Sea?\\n',\n",
       "   'response': 'Yoroi Akado and Misumi Tsurugi started work for him, and together they started to steal a cargo ship that the Land of Water sent to the Land of Sea',\n",
       "   'expected_output': None},\n",
       "  0.05505138263106346),\n",
       " ({'name': None,\n",
       "   'instruction': 'Answer the question',\n",
       "   'guideline': '',\n",
       "   'input': 'What did Aniki plan to do with Himawari when he recognized her?\\n',\n",
       "   'response': 'Aniki planned to sell Himawari to someone who hated the Seventh Hokage (Naruto Uzumaki).',\n",
       "   'expected_output': None},\n",
       "  0.09337256848812103)]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_quality_prompts[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"high_quality_deduplicated_synthetic_qa_prompts.jsonl\") as f:\n",
    "    prompts = [json.loads(l) for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def is_multiple_question(question):\n",
    "    if \"A.\" in question or \"B.\" in question or \"C.\" in question or \"D.\" in question:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "multiple_choices_instructions = [\n",
    "    \"Answer multiple-choice questions\",\n",
    "    \"Answer multiple-choice questions below\",\n",
    "    \"Answer the following multiple-choice questions\",\n",
    "    \"Answer the multiple-choice questions\",\n",
    "]\n",
    "question_answer_instructions = [\n",
    "    \"Answer the question\",\n",
    "    \"Answer the following question\",\n",
    "    \"Answer the question below\",\n",
    "    \"Provide the most relevant answer to specific question\",\n",
    "]\n",
    "\n",
    "def lpoprpush(arr):\n",
    "    item = arr[0]\n",
    "    arr.pop(0)\n",
    "    arr.append(item)\n",
    "    return item\n",
    "\n",
    "def change_instruction(prompt):\n",
    "    if is_multiple_question(prompt[\"input\"]):\n",
    "        prompt[\"instruction\"] = lpoprpush(multiple_choices_instructions)\n",
    "    else:\n",
    "        prompt[\"instruction\"] = lpoprpush(question_answer_instructions)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompts = [change_instruction(prompt) for prompt in prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"high_quality_deduplicated_synthetic_qa_prompts.jsonl\", \"w\") as f:\n",
    "    for prompt in new_prompts:\n",
    "        f.write(json.dumps(prompt) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "naruto_qa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
